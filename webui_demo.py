# coding:utf-8
import json
import os
import time

import gradio as gr
import torch
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

cache_dir = './'

snapshot_download('X-D-Lab/MindChat-Qwen-7B',
                  cache_dir=cache_dir,
                  revision='v1.0.0')

tokenizer = AutoTokenizer.from_pretrained(cache_dir +
                                          "X-D-Lab/MindChat-Qwen-7B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(cache_dir +
                                          "X-D-Lab/MindChat-Qwen-7B", device_map="auto", trust_remote_code=True, fp16=True).eval()
model.generation_config = GenerationConfig.from_pretrained(cache_dir +
                                          "X-D-Lab/MindChat-Qwen-7B", trust_remote_code=True) 



title = "ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹"

description = """
ğŸ” MindChat(æ¼«è°ˆ): æ—¨åœ¨é€šè¿‡è¥é€ è½»æ¾ã€å¼€æ”¾çš„äº¤è°ˆç¯å¢ƒ, ä»¥æ”¾æ¾èº«å¿ƒã€äº¤æµæ„Ÿå—æˆ–åˆ†äº«ç»éªŒçš„æ–¹å¼, ä¸ºç”¨æˆ·æä¾›éšç§ã€æ¸©æš–ã€å®‰å…¨ã€åŠæ—¶ã€æ–¹ä¾¿çš„å¯¹è¯ç¯å¢ƒ, ä»è€Œå¸®åŠ©ç”¨æˆ·å…‹æœå„ç§å›°éš¾å’ŒæŒ‘æˆ˜, å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•.

ğŸ¦Š æ— è®ºæ˜¯åœ¨å·¥ä½œåœºæ‰€è¿˜æ˜¯åœ¨ä¸ªäººç”Ÿæ´»ä¸­, MindChatæœŸæœ›é€šè¿‡è‡ªèº«çš„åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†, åœ¨ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹, å…¨æ—¶æ®µå…¨å¤©å€™ä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¿ƒç†é™ªä¼´å’Œå€¾å¬, åŒæ—¶å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•, ä»¥æœŸä¸ºå»ºè®¾ä¸€ä¸ªæ›´åŠ å¥åº·ã€åŒ…å®¹å’Œå¹³ç­‰çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡.

ğŸ™…â€ ç›®å‰ï¼ŒMindChatè¿˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å¿ƒç†åŒ»ç”Ÿå’Œå¿ƒç†å’¨è¯¢å¸ˆï¼Œæ— æ³•åšå‡ºä¸“ä¸šçš„å¿ƒç†è¯Šæ–­æŠ¥å‘Šã€‚è™½MindChatåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æè‡´æ³¨é‡æ¨¡å‹å®‰å…¨å’Œä»·å€¼è§‚æ­£å‘å¼•å¯¼ï¼Œä½†ä»æ— æ³•ä¿è¯æ¨¡å‹è¾“å‡ºæ­£ç¡®ä¸”æ— å®³ï¼Œå†…å®¹ä¸Šæ¨¡å‹ä½œè€…åŠå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚

ğŸ‘ æ›´ä¸ºä¼˜è´¨ã€å®‰å…¨ã€æ¸©æš–çš„æ¨¡å‹æ­£åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œæ¬¢è¿å…³æ³¨ï¼š[MindChat Github](https://github.com/X-D-Lab/MindChat)
"""
submit_btn = 'å‘é€'
retry_btn = 'ğŸ”„ é‡æ–°ç”Ÿæˆ'
undo_btn = 'â†©ï¸ æ’¤é”€'
clear_btn = 'ğŸ—‘ï¸ æ¸…é™¤å†å²'


def predict(message, history):

    if history is None:
        history = []
    history = history[-20:]
    response, history = model.chat(tokenizer, message, history=history) 

    history.append((message, response))

    for i in range(len(response)):
        time.sleep(0.02)
        yield response[:i + 1]


demo = gr.ChatInterface(predict,
                        title=title,
                        description=description,
                        cache_examples=True,
                        submit_btn=submit_btn,
                        retry_btn=retry_btn,
                        clear_btn=clear_btn,
                        undo_btn=undo_btn).queue()

demo.launch()