# coding:utf-8
import json
import os
import time

import gradio as gr
import torch
from modelscope.hub.snapshot_download import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig

device = "cuda" # the device to load the model onto

cache_dir = './'

model_id = "X-D-Lab/MindChat-Qwen2-4B"

snapshot_download(model_id, cache_dir=cache_dir)

model_path = cache_dir + model_id

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

title = "ğŸ‹MindChat: æ¼«è°ˆå¿ƒç†å¤§æ¨¡å‹"

description = """
ğŸ” MindChat(æ¼«è°ˆ): æ—¨åœ¨é€šè¿‡è¥é€ è½»æ¾ã€å¼€æ”¾çš„äº¤è°ˆç¯å¢ƒ, ä»¥æ”¾æ¾èº«å¿ƒã€äº¤æµæ„Ÿå—æˆ–åˆ†äº«ç»éªŒçš„æ–¹å¼, ä¸ºç”¨æˆ·æä¾›éšç§ã€æ¸©æš–ã€å®‰å…¨ã€åŠæ—¶ã€æ–¹ä¾¿çš„å¯¹è¯ç¯å¢ƒ, ä»è€Œå¸®åŠ©ç”¨æˆ·å…‹æœå„ç§å›°éš¾å’ŒæŒ‘æˆ˜, å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•.

ğŸ¦Š æ— è®ºæ˜¯åœ¨å·¥ä½œåœºæ‰€è¿˜æ˜¯åœ¨ä¸ªäººç”Ÿæ´»ä¸­, MindChatæœŸæœ›é€šè¿‡è‡ªèº«çš„åŠªåŠ›å’Œä¸“ä¸šçŸ¥è¯†, åœ¨ä¸¥æ ¼ä¿æŠ¤ç”¨æˆ·éšç§çš„å‰æä¸‹, å…¨æ—¶æ®µå…¨å¤©å€™ä¸ºç”¨æˆ·æä¾›å…¨é¢çš„å¿ƒç†é™ªä¼´å’Œå€¾å¬, åŒæ—¶å®ç°è‡ªæˆ‘æˆé•¿å’Œå‘å±•, ä»¥æœŸä¸ºå»ºè®¾ä¸€ä¸ªæ›´åŠ å¥åº·ã€åŒ…å®¹å’Œå¹³ç­‰çš„ç¤¾ä¼šè´¡çŒ®åŠ›é‡.

ğŸ™…â€ ç›®å‰ï¼ŒMindChatè¿˜ä¸èƒ½æ›¿ä»£ä¸“ä¸šçš„å¿ƒç†åŒ»ç”Ÿå’Œå¿ƒç†å’¨è¯¢å¸ˆï¼Œæ— æ³•åšå‡ºä¸“ä¸šçš„å¿ƒç†è¯Šæ–­æŠ¥å‘Šã€‚è™½MindChatåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æè‡´æ³¨é‡æ¨¡å‹å®‰å…¨å’Œä»·å€¼è§‚æ­£å‘å¼•å¯¼ï¼Œä½†ä»æ— æ³•ä¿è¯æ¨¡å‹è¾“å‡ºæ­£ç¡®ä¸”æ— å®³ï¼Œå†…å®¹ä¸Šæ¨¡å‹ä½œè€…åŠå¹³å°ä¸æ‰¿æ‹…ç›¸å…³è´£ä»»ã€‚

ğŸ‘ æ›´ä¸ºä¼˜è´¨ã€å®‰å…¨ã€æ¸©æš–çš„æ¨¡å‹æ­£åœ¨èµ¶æ¥çš„è·¯ä¸Šï¼Œæ¬¢è¿å…³æ³¨ï¼š[MindChat Github](https://github.com/X-D-Lab/MindChat)
"""

submit_btn = 'å‘é€'
retry_btn = 'ğŸ”„ é‡æ–°ç”Ÿæˆ'
undo_btn = 'â†©ï¸ æ’¤é”€'
clear_btn = 'ğŸ—‘ï¸ æ¸…é™¤å†å²'



def predict(message,history):
    if history is None:
        history = []
    history = history[-20:]

    # å°†history è½¬ä¸ºmeaaages
    messages = [{"role": "system", "content": "You are a helpful assistant."}]
    for i in history:
        messages.append({"role": "user", "content": i[0]})
        messages.append({"role": "assistant", "content": i[1]})
    messages.append({"role": "user", "content": message})

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=512
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    history.append((message, response))
    for i in range(len(response)):
        time.sleep(0.02)
        yield response[:i + 1]


demo = gr.ChatInterface(predict,
                        title=title,
                        description=description,
                        cache_examples=True,
                        submit_btn=submit_btn,
                        retry_btn=retry_btn,
                        clear_btn=clear_btn,
                        undo_btn=undo_btn).queue()

demo.launch()